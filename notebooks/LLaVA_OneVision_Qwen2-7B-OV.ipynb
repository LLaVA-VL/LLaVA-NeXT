{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47fc642",
   "metadata": {},
   "source": [
    "# LLaVA_OneVision Script\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da161985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def main():\n",
    "    pretrained = \"lmms-lab/llava-onevision-qwen2-7b-ov\"\n",
    "    model_name = \"llava_qwen\"\n",
    "    device = \"cuda\"\n",
    "    device_map = \"auto\"\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)\n",
    "\n",
    "    model.eval()\n",
    "    model.tie_weights()\n",
    "\n",
    "    # Load the image\n",
    "    url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "\n",
    "    conv_template = \"qwen_1_5\"\n",
    "    question = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\n",
    "    conv = copy.deepcopy(conv_templates[conv_template])\n",
    "    conv.append_message(conv.roles[0], question)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_question = conv.get_prompt()\n",
    "\n",
    "    input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "#    attention_mask = torch.ones_like(input_ids)\n",
    "    image_sizes = [image.size]\n",
    "\n",
    "    # Generate the output\n",
    "    cont = model.generate(\n",
    "        input_ids,\n",
    "#       attention_mask=attention_mask,\n",
    "        images=image_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "    print(text_outputs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b75c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
