{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25abef6",
   "metadata": {},
   "source": [
    "# Interleaved Model Script\n",
    ".\n",
    ".\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3bf2403-d4ca-48d4-b5f9-db6184c6f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/nareknurijanyan/miniforge3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/auto/home/nareknurijanyan/miniforge3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afe7cfb-5384-4508-8042-9d63a73bab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05377047-726b-4bbe-8954-a7fa0668747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLaVA model: lmms-lab/llava-next-interleave-qwen-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/nareknurijanyan/miniforge3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config with {'image_aspect_ratio': 'pad'}\n",
      "Loading vision tower: google/siglip-so400m-patch14-384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Class: LlavaQwenForCausalLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlavaQwenForCausalLM(\n",
       "  (model): LlavaQwenModel(\n",
       "    (embed_tokens): Embedding(151646, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2FlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "    (vision_tower): SigLipVisionTower(\n",
       "      (vision_tower): SigLipVisionModel(\n",
       "        (vision_model): SigLipVisionTransformer(\n",
       "          (embeddings): SigLipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(729, 1152)\n",
       "          )\n",
       "          (encoder): SigLipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-25): 26 x SigLipEncoderLayer(\n",
       "                (self_attn): SigLipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SigLipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (head): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (vision_resampler): IdentityMap()\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1152, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151646, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "pretrained = \"lmms-lab/llava-next-interleave-qwen-7b\"\n",
    "model_name = \"llava_qwen\"\n",
    "device = torch.device(\"cuda:1\")\n",
    "device_map = {\"\": device}  # Explicitly map model to this device\n",
    "llava_model_args = {\n",
    "    \"multimodal\": True,\n",
    "}\n",
    "overwrite_config = {}\n",
    "overwrite_config[\"image_aspect_ratio\"] = \"pad\"\n",
    "llava_model_args[\"overwrite_config\"] = overwrite_config\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map, **llava_model_args)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ec1b91-415e-4b32-b4dd-198d2bcd7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/nfs/ap/mnt/sxtn/AerialVLN/DATA/rgb_images/val_unseen/301KG0KXAKJZJRV104QUVPLUL5F2HR/0.npy\"\n",
    "path2 = \"/nfs/ap/mnt/sxtn/AerialVLN/DATA/rgb_images/val_unseen/301KG0KXAKJZJRV104QUVPLUL5F2HR/1.npy\"\n",
    "\n",
    "url1 = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "url2 = \"https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/llava_logo.png\"\n",
    "\n",
    "# image1 = np.load(path1)\n",
    "# image2 = np.load(path2)\n",
    "\n",
    "image1 = Image.open(requests.get(url1, stream=True).raw)\n",
    "image2 = Image.open(requests.get(url2, stream=True).raw)\n",
    "\n",
    "# image1 = Image.fromarray(image1.astype('uint8'))\n",
    "# image2 = Image.fromarray(image2.astype('uint8'))\n",
    "\n",
    "images = [image1, image2]\n",
    "image_tensors = process_images(images, image_processor, model.config)\n",
    "\n",
    "image_tensors = [_image.to(dtype=torch.float16, device=device) for _image in image_tensors]\n",
    "\n",
    "instruction = \"Take off, fly through the tower of cable bridge and down to the end of the road. Turn left, fly over the five-floor building with a yellow shop sign and down to the intersection on the left. Head to the park and turn right, fly along the edge of the park. March forward, at the intersection turn right, and finally land in front of the building with a red billboard on its rooftop\"\n",
    "description = \"You are navigating a flying drone based on visual input and instructions. Analyze the current and previous images to determine the correct action to take from the provided options. Use the instruction and the differences between the images to guide your choice.\"\n",
    "conv_template = \"qwen_1_5\"\n",
    "\n",
    "question = f\"\"\"\n",
    "[Description] {description} [/Description]\n",
    "[INSTR] {instruction} [/INSTR]\n",
    "[ACTIONS] {DEFAULT_IMAGE_TOKEN} [/ACTIONS]\n",
    "[CurrentScene] {DEFAULT_IMAGE_TOKEN} [/CurrentScene]\n",
    "\n",
    "[Choice]\n",
    "A) Move Forward\n",
    "B) Turn Left \n",
    "C) Turn Right\n",
    "D) Ascend \n",
    "E) Descend\n",
    "F) Move Left\n",
    "G) Move Right \n",
    "H) Stop. \n",
    "[/Choice]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7144fd-d759-4861-94ff-a50f628f8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6a5d39-76d7-413f-9f3a-f2ab434e2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957a757a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "cont = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensors,\n",
    "    image_sizes=image_sizes,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "print(text_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ff31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
